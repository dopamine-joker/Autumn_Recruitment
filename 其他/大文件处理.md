https://blog.csdn.net/qq_33666011/article/details/104994641

# **密匙一、分而治之/Hash映射 + Hash_map统计 + 堆/快速/归并排序**

基本思路就是先通过hash，然后取模，取模目的是为了让相同的数据都能放在同一个文件进行处理，然后hash_map分别统计每一个小文件的情况，在通过堆来维护最终多个小文件的合并结果。

这里注意其中几个步骤的作用

1. hash取模，分小文件，并且相同数据只能出现在同一个小文件(eg. hash % 4000)  [分组思想，比较重要]
2. hash_map或hash_set，计数或找重复
3. 堆，快排... 一般用来单文件或多文件找top k。

## 1. 海量ip找访问最多的top k个

**具体思路**

分治法就是“化大为小”，“化单为多”，我们可以将所有IP分放在100个文件中，然后分别统计每个文件的topK。

> 但是需要注意的是， 必须保证每种IP地址只在一个文件中出现，比如我们可以采用模100的算法，将0,1,2,3,4...分别放入一百个文件中，然后使用HashMap分别统计每个文件中IP出现的次数。

到这步之后，如果我们需要统计所有文件的topK，可以采用最小堆的方式。具体的做法是，首先用K个数据构建最小堆，后面的数据依次判断是否入堆，如果入堆则进行调整，最后得到的就是次数最多的一百个IP。

**总结**

（1）将ip地址放入多个小文件中，保证每种IP只出现在一个文件中
（2）利用hashmap统计每个小文件中IP出现的次数
（3）利用最小堆得到所有IP访问次数最多的100个

## 2. 单词频数

有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。

1. 分而治之/hash映射：顺序读文件中，对于每个词x，取hash(x)%5000，然后按照该值存到5000个小文件（记为x0,x1,...x4999）中。这样每个文件大概是200k左右。如果其中的有的文件超过了1M大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过1M。
2. hash_map统计：对每个小文件，采用trie树/hash_map等统计每个文件中出现的词以及相应的频率。
3. 堆/归并排序：取出出现频率最大的100个词（可以用含100个结点的最小堆）后，再把100个词及相应的频率存入文件，这样又得到了5000个文件。最后就是把这5000个文件进行归并（类似于归并排序）的过程了。

## 3. 数据分布不同机器求TOP

海量数据分布在100台电脑中，想个办法高效统计出这批数据的TOP10。

- 情况一，相同数据分布在同一台机器

    和上述一样套路，在每一台机器中，先hash，后统计，每台机器取top10，然后全部机器的top10比较

- 情况二，相同数据可以分布在不同机器

    遍历一遍所有数据，重新hash取摸，如此使得同一个元素只出现在单独的一台电脑中，然后采用上面所说的方法，统计每台电脑中各个元素的出现次数找出TOP10，继而组合100台电脑上的TOP10，找出最终的TOP10。

## 4. 大文件找重复

给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？

对两个文件进行hash，各自分成多个小文件(a0,a1,a2.... b0,b1,b2...)，这样相同的url只能出现在对应的小文件((a0, b0), (a1, b1), (a2, b2))...

再分别对这些对应文件hash_set找重就好

1. 分而治之/hash映射

    遍历文件a，对每个url求取，然后根据所取得的值将url分别存储到1000个小文件（记为，这里漏写个了a1）中。这样每个小文件的大约为300M。遍历文件b，采取和a相同的方式将url分别存储到1000小文件中（记为）。这样处理后，所有可能相同的url都在对应的小文件（）中，不对应的小文件不可能有相同的url。然后我们只要求出1000对小文件中相同的url即可。

2. hash_set统计：求每对小文件中相同的url时，可以把其中一个小文件的url存储到hash_set中。然后遍历另一个小文件的每个url，看其是否在刚才构建的hash_set中，如果是，那么就是共同的url，存到文件里面就可以了。

# 密匙二、多层划分

多层划分----其实本质上还是分而治之的思想，重在“分”的技巧上！
 适用范围：第k大，中位数，不重复或重复的数字
 基本原理及要点：因为元素范围很大，不能利用直接寻址表，所以通过多次划分，逐步确定范围，然后最后在一个可以接受的范围内进行。

## 1. 大量数据找非重复

2.5亿个整数中找出不重复的整数的个数，内存空间不足以容纳这2.5亿个整数。

有点像鸽巢原理，整数个数为2^32,也就是，我们可以将这2^32个数，划分为2^8个区域(比如用单个文件代表一个区域)，然后将数据分离到不同的区域，然后不同的区域在利用**bitmap**就可以直接解决了。也就是说只要有足够的磁盘空间，就可以很方便的解决。

## 2. n亿个int找它们的中位数。

这个例子比上面那个更明显。首先我们将int划分为2^16个区域，然后读取数据统计落到各个区域里的数的个数，之后我们根据统计结果就可以判断中位数落到那个区域，同时知道这个区域中的第几大数刚好是中位数。然后第二次扫描我们只统计落在这个区域中的那些数就可以了。

思路大概就是找到第n/2亿个数，首先先把int划分区域，然后统计区域里的数，快速收敛到存在中位数的区间，然后再计算即可。

# 密匙三：Bloom filter/Bitmap（布隆过滤器/位图）

注意，布隆过滤器有一定的错误率。但一般出错概率比较低。

## 1. url找重复

给你A,B两个文件，各存放50亿条URL，每条URL占用64字节，内存限制是4G，让你找出A,B文件共同的URL。如果是三个乃至n个文件呢？

根据这个问题我们来计算下内存的占用，4G=2^32大概是40亿*8大概是340亿，n=50亿，如果按出错率0.01算需要的大概是650亿个bit。现在可用的是340亿，相差并不多，这样可能会使出错率上升些。另外如果这些urlip是一一对应的，就可以转换成ip，则大大简单了。

## 2. 大量数据找非重复

同上，先分组，然后对每一个组使用bitmap解决即可。

# 密匙四、Trie树/数据库/倒排索引

# 密匙五、[外部排序](https://blog.csdn.net/ailunlee/article/details/84548950)

给你一个包含20亿个int类型整数的文件，计算机的内存只有2GB，怎么给它们排序？一个int数占4个字节，20个亿需要80亿字节，大概占用8GB的内存，而计算机只有2GB的内存，数据都装不下！可以把8GB分割成4个2GB的数据来排，然后在把他们拼凑回去。如下图：

![img](https://img-blog.csdnimg.cn/20181126161926976.jpeg)

排序的时候可以选择**快速排序**或**归并排序**等算法。为了方便，我们把排序好的2G有序数据称为有序子串。接着把两个小的有序子串合并成一个大的有序子串。

![img](https://img-blog.csdnimg.cn/20181126161926964.jpeg)

注意:读取的时候是每次读取一个int数，通过比较之后再输出。注意这里可以把2G内存写完，在写到硬盘中去，这样可以减少读取硬盘的次数。

按照这个方法来回合并，总共经过三次合并之后就可以得到8G的有序子串。

