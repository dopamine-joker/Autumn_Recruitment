# 1. go_chat

## 1. RPC原理

原文链接: https://www.cnblogs.com/LBSer/p/4853234.html

1. **如何调用远程服务**

    　由于各服务部署在不同机器，服务间的调用免不了网络通信过程，服务消费方每调用一个服务都要写一坨网络通信相关的代码，不仅复杂而且极易出错。

     如果有一种方式能让我们像调用本地服务一样调用远程服务，而让调用者对网络通信这些细节透明，那么将大大提高生产力。
     
     要让网络通信细节对使用者透明，我们需要对通信细节进行封装，我们先看下一个RPC调用的流程涉及到哪些通信细节：

    ![img](https://images2015.cnblogs.com/blog/522490/201510/522490-20151003120412386-363334260.png)

    > 一个完整的RPC架构里面包含了四个核心的组件，分别是Client，Client Stub，Server以及Server Stub，这个Stub可以理解为存根。
    >
    > - 客户端(Client)，服务的调用方。
    > - 客户端存根(Client Stub)，存放服务端的地址消息，再将客户端的请求参数打包成网络消息，然后通过网络远程发送给服务方。
    > - 服务端(Server)，真正的服务提供者。
    > - 服务端存根(Server Stub)，接收客户端发送过来的消息，将消息解包，并调用本地的方法。

    1）服务消费方（client）调用以本地调用方式调用服务；

    2）client stub接收到调用后负责将方法、参数等组装成能够进行网络传输的消息体；

    3）client stub找到服务地址，并将消息发送到服务端；

    4）server stub收到消息后进行解码；

    5）server stub根据解码结果调用本地的服务；

    6）本地服务执行并将结果返回给server stub；

    7）server stub将返回结果打包成消息并发送至消费方；

    8）client stub接收到消息，并进行解码；

    9）服务消费方得到最终结果。

    RPC的目标就是要**2~8**这些步骤都封装起来，让用户对这些细节透明。

2. **如何透明化远程服务调用**

    ​	怎么封装通信细节才能让用户像以本地调用方式调用远程服务呢？对java来说就是使用代理！java代理有两种方式：1） **jdk 动态代理**；2）**字节码生成**。尽管字节码生成方式实现的代理更为强大和高效，但代码维护不易，大部分公司实现RPC框架时还是选择**动态代理**方式。

    ​	下面简单介绍下动态代理怎么实现我们的需求。我们需要实现RPCProxyClient代理类，代理类的invoke方法中封装了与远端服务通信的细节，消费方首先从RPCProxyClient获得服务提供方的接口，当执行helloWorldService.sayHello("test")方法时就会调用invoke方法。

    ```java
    public class RPCProxyClient implements java.lang.reflect.InvocationHandler{
        private Object obj;
    
        public RPCProxyClient(Object obj){
            this.obj=obj;
        }
    
        /**
         * 得到被代理对象;
         */
        public static Object getProxy(Object obj){
            return java.lang.reflect.Proxy.newProxyInstance(obj.getClass().getClassLoader(),
                    obj.getClass().getInterfaces(), new RPCProxyClient(obj));
        }
    
        /**
         * 调用此方法执行
         */
        public Object invoke(Object proxy, Method method, Object[] args)
                throws Throwable {
            //结果参数;
            Object result = new Object();
            // ...执行通信相关逻辑
            // ...
            return result;
        }
    }
    ```

    ```java
    public class Test {
        public static void main(String[] args) {
            HelloWorldService helloWorldService = (HelloWorldService)RPCProxyClient.getProxy(HelloWorldService.class);
            helloWorldService.sayHello("test");
        }
    }
    ```

3. **RPC结构拆解**

    ![img](https://images0.cnblogs.com/blog2015/569491/201503/080911415868394.png)

    RPC 服务方通过 `RpcServer` 去导出（export）远程接口方法，而客户方通过 `RpcClient` 去引入（import）远程接口方法。 客户方像调用本地方法一样去调用远程接口方法，RPC 框架提供接口的代理实现，实际的调用将委托给代理 `RpcProxy` 。 代理封装调用信息并将调用转交给 `RpcInvoker` 去实际执行。 在客户端的 `RpcInvoker` 通过连接器 `RpcConnector` 去维持与服务端的通道 `RpcChannel`， 并使用 `RpcProtocol` 执行协议编码（encode）并将编码后的请求消息通过通道发送给服务方。

    RPC 服务端接收器 `RpcAcceptor` 接收客户端的调用请求，同样使用 `RpcProtocol` 执行协议解码（decode）。 解码后的调用信息传递给 `RpcProcessor` 去控制处理调用过程，最后再委托调用给 `RpcInvoker` 去实际执行并返回调用结果。

4. **如何对消息进行编码解码**

    上节讲了invoke里需要封装通信细节，而通信的第一步就是要确定客户端和服务端相互通信的消息结构。客户端的请求消息结构一般需要包括以下内容：

    1. 接口名
    2. 方法名
    3. 参数类型，参数值
    4. 超时时间
    5. **requestID**

    返回值

    1. 状态code
    2. **requestID**

    确定这些内容后进行**序列化**和**反序列化**传输

    目前互联网公司广泛使用Protobuf、Thrift、Avro等成熟的序列化解决方案来搭建RPC框架，这些都是久经考验的解决方案。

4. **为什么要有requestID**

    如果使用netty的话，一般会用channel.writeAndFlush()方法来发送消息二进制串，这个方法调用后对于整个远程调用(从发出请求到接收到结果)来说是一个异步的，即对于当前线程来说，将请求发送出来后，线程就可以往后执行了，至于服务端的结果，是服务端处理完成后，再以消息的形式发送给客户端的。于是这里出现以下两个问题：

    1）怎么让当前线程“暂停”，等结果回来后，再向后执行？

    2）如果有多个线程同时进行远程方法调用，这时建立在client server之间的socket连接上会有很多双方发送的消息传递，前后顺序也可能是随机的，server处理完结果后，将结果消息发送给client，client收到很多消息，怎么知道哪个消息结果是原先哪个线程调用的？

    　如下图所示，线程A和线程B同时向client socket发送请求requestA和requestB，socket先后将requestB和requestA发送至server，而server可能将responseA先返回，尽管requestA请求到达时间更晚。我们需要一种机制保证responseA丢给ThreadA，responseB丢给ThreadB。

     ![img](https://images2015.cnblogs.com/blog/522490/201510/522490-20151003171953574-1892668698.png)

    　　怎么解决呢？

    1）client线程每次通过socket调用一次远程接口前，生成一个唯一的ID，即requestID（requestID必需保证在一个Socket连接里面是唯一的），一般常常使用AtomicLong从0开始累计数字生成唯一ID；

    2）将处理结果的回调对象callback，存放到全局ConcurrentHashMap里面put(requestID, callback)；

    3）当线程调用channel.writeAndFlush()发送消息后，紧接着执行callback的get()方法试图获取远程返回的结果。在get()内部，则使用**synchronized**获取回调对象callback的锁，再先检测是否已经获取到结果，如果没有，然后调用callback的wait()方法，释放callback上的锁，让当前线程处于等待状态。

    4）服务端接收到请求并处理后，将response结果（此结果中包含了前面的requestID）发送给客户端，客户端socket连接上专门监听消息的线程收到消息，分析结果，取到requestID，再从前面的ConcurrentHashMap里面get(requestID)，从而找到callback对象，再用synchronized获取callback上的锁，将方法调用结果设置到callback对象里，再调用callback.notifyAll()唤醒前面处于等待状态的线程。

    ```java
    public Object get() {
            synchronized (this) { // 旋锁
                while (!isDone) { // 是否有结果了
                    wait(); //没结果是释放锁，让当前线程处于等待状态
                }
            }
        }
    ```

    ```java
    private void setDone(Response res) {
            this.res = res;
            isDone = true;
            synchronized (this) { //获取锁，因为前面wait()已经释放了callback的锁了
                notifyAll(); // 唤醒处于等待的线程
            }
        }
    ```

5. **如何发布服务**

    为了能实现自动告知，即机器的增添、剔除对调用方透明，调用者不再需要写死服务提供方地址？当然可以，现如今zookeeper被广泛用于实现服务自动注册与发现功能！

    ![img](https://images2015.cnblogs.com/blog/522490/201510/522490-20151003183747543-2138843838.png)

    具体来说，zookeeper就是个分布式文件系统，每当一个服务提供者部署后都要将自己的服务注册到zookeeper的某一路径上: /{service}/{version}/{ip:port}, 比如我们的HelloWorldService部署到两台机器，那么zookeeper上就会创建两条目录：分别为/HelloWorldService/1.0.0/100.19.20.01:16888  /HelloWorldService/1.0.0/100.19.20.02:16888。

    　　zookeeper提供了“心跳检测”功能，它会定时向各个服务提供者发送一个请求（实际上建立的是一个 Socket 长连接），如果长期没有响应，服务中心就认为该服务提供者已经“挂了”，并将其剔除，比如100.19.20.02这台机器如果宕机了，那么zookeeper上的路径就会只剩/HelloWorldService/1.0.0/100.19.20.01:16888。

    　　服务消费者会去监听相应路径（/HelloWorldService/1.0.0），一旦路径上的数据有任务变化（增加或减少），zookeeper都会通知服务消费方服务提供者地址列表已经发生改变，从而进行更新。

    　　更为重要的是zookeeper与生俱来的容错容灾能力（比如leader选举），可以确保服务注册表的高可用性。

## 2. Raft

示意图[http://thesecretlivesofdata.com/raft/]

1. **节点三个状态**

    - follower 与Leader保持同步
    - leader 与客户端交互
    - candidate 选举状态，若选举成功则成为Leader

    ![image-20210910171801074](https://cdn.jsdelivr.net/gh/dopamine-joker/image-host/image/image-20210910171801074.png)

2. **初始状态**

    一般初始状态全部节点都是follower状态， 此时没有Leader。

3. **Leader Election（初始化状态）**

    由于follower没有听到leader发来的心跳包(此时没有leader)，timeout时间后就会进入candidate状态。此时candidate节点先给自己投上一票，然后**发送投票请求到其他所有节点**。其他节点收到后，重置自己的timeout时间，然后返回投票响应给candidate。此时若candidate得到了**超过一半的节点的投票**，则其成为Leader。

4. **Log Replication**

    现在发向集群的请求都要经由leader处理了。每个请求都会作为一个entry加入到节点的log中。在leader收到请求时，他还并**不会立马提交这个更新**，而是将这个entry发到其他的每一个follower节点中，follower收到消息后就处理这个entry加入到log中,并响应返回给leader,若leader收到了超过半数follower的回应，则认为更新成功，**提交数据更新**，并把**确定更新这个消息**再次发给所有的follower，把成功结果返回给客户端。

5. **Leader Election**

    在raft中有两种情况来控制Leader Election。

    - **一次term选举即选出leader**

        每一个follower都会随机设置一个election timeout时间，一般在150ms~300ms。这就意味着每一个follower的election timeout是不一样的。流程就同第3点的一样。如果此时ledaer宕掉，则剩下的follower由于超时会重新选举。宕掉的节点在恢复后会同步新leader的term和log。每次选主都会形成一个唯一的TERM编号，相当于逻辑时钟。每一条日志都有全局唯一的编号。

    - **一个term中出现平票**

        如果一次选举中出现了**平票**的情况，则不会产生leader，进入下一次选举。并且在重新选举leader后，其会在每一次的heart beat中同步log。

6.  **Log Replication**

    注意网络被隔离的情况下，即多个节点被划分为几个互不相同的区域。经过一段时间内每一个区域都有一个leaedr。

    ![image-20210910171149330](https://cdn.jsdelivr.net/gh/dopamine-joker/image-host/image/image-20210910171149330.png)

    ![image-20210910171215550](https://cdn.jsdelivr.net/gh/dopamine-joker/image-host/image/image-20210910171215550.png)

    注意此时发送给两个leader的数据，下面的会同步失败，因为没有获得超过半数的follower的回应。

    ![image-20210910171302915](https://cdn.jsdelivr.net/gh/dopamine-joker/image-host/image/image-20210910171302915.png)

    恢复后，节点B看到节点C拥有更高的term,所以此时B和A将会回滚之前的entry，然后同步新leader（节点C）的log。

    ![image-20210910171904186](https://cdn.jsdelivr.net/gh/dopamine-joker/image-host/image/image-20210910171904186.png)

    这里第三个follower出现entry6有可能是因为上面提到的，网络隔离情况下，客户端写的entry没有同步成功，造成了某个follower有高于leader 的entry的情况。

    ![image-20210910171919849](https://cdn.jsdelivr.net/gh/dopamine-joker/image-host/image/image-20210910171919849.png)

    

## 3. etcd原理

https://www.cnblogs.com/elnino/p/9509422.html

https://www.lixueduan.com/post/etcd/06-why-mvcc/

https://blog.csdn.net/weixin_39517868/article/details/112670403

https://www.cnblogs.com/xigang8068/p/5786027.html

 1. **ETCD架构**

    ![img](https://www.ipcpu.com/wp-content/uploads/2018/01/wpid-dcbcc33700190519c7c8e5c57b5a4648_6f3eb749-4a50-49f4-9493-94e235a03343.jpg)

    - HTTP Server

        用于处理用户发送的API请求和其他etcd节点的同步与心跳信息请求

    - store

        用于处理etcd支持的各类功能的事务，包括数据索引，节点状态变更，监控与反馈，事件处理与执行等等，是etcd对用户提供的大多数API功能的具体实现。

    - Raft

        Raft强一致性算法的具体实现，是etcd的核心

    - WAL

        Write Ahead Log(预写式日志)，是etcd的数据存储方式。除了**在内存中存储所有数据的状态以及节点的索引以外**，etcd就通过WAL进行**持久化存储**。WAL中，所有的数据提交之前都会事先记录日志。Snapshot是为了防止数据过多而进行的状态快照。Entry标识存储的具体日志内容。

    通常，一个用户的请求发送过来，会经由 HTTP Server 转发给 Store 进行具体的事务处理，如果涉及到节点的修改，则交给 Raft 模块进行状态的变更、日志的记录，然后再同步给别的 etcd节点以确认数据提交，最后进行数据的提交，再次同步。

 2. **ETCD V2**

    etcd v2 是一个纯内存数据库，写操作先通过 Raft 复制日志文件，复制成功后将数据写入内存。整个数据库在内存中是一个简单的树结构，并未实时地将数据写入磁盘。

    持久化是靠快照来实现的，具体实现就是将整个内存中的数据复制一份出来，然后序列化成 JSON ，写入磁盘中，成为一个快照。

3. **ETCD V3**

    etcd v3 将数据存储在一个多版本的持久化 key-value 存储里面。值得注意的是，作为 key value 存储的 etcd 会将数据存储在另一个 key-value 数据库中。

    当持久键值存储的值发生变化时，持久化键值存储将保存先前版本的键值对，etcd 后台的键值存储实际上是不可变的， etcd 操作不会就地更新结构，而是始终生成一个更新之后的结构。 发生修改后， key 先前版本的所有值仍然可以访问和 watch。

    为了防止数据存储随着时间的推移无限期增长，并且为了维护旧版本， etcd 可能 压缩（删除） key 的旧版本数据。

 4. **存储结构**

    etcd 将物理数据存储为一棵持久**`B+树`**中的键值对。为了高效，每个revision 存储状态都只包含相对于之前 revision 的增量。 一个 revision 可能对应于树中的多key。

    B＋树中键值对的 **key**即**revision**, revision 元组（ main, sub ），其中main 是该 revision 的主版本号， sub 是同一revision 的副版本号，其用于区分 一个 revision 不同 key。

    B＋树按 key 字典字节序进行排序。这样， etcd revision 增量的范围查询（ range query ，即从某个 revision 另一个 revision ）会很快一一因为我们记录了从一个特定 revision到另一个revision 的修改量。

    etcd v3 还在**内存**中维护了一个基于**`B树`**的二级索引来加快对 key 的范围查询。该B树的 key 是向用户暴露的 etcd v3 存储的 key ，而该B树索引的value 则是一个指向上文讨论的持久化 B＋树的增量的指针。

 5. **BoltDB**

    etcd底层的纯go实现的kv存储引擎。etcd 内部使用`BoltDB`存储数据。**BoltDB 只提供简单的 key value 存储**，没有其他的特性，因此 BoltDB 可以做到代码精简(小于3KB,质量高，非常适合以BoltDB 为基础在其之上构建更加复杂的数据库功能。

    (1) **数据如何存储**

    ```go
    type revision struct {
     // main is the main revision of a set of changes that happen atomically.
     main int64
     // sub is the sub revision of a change in a set of changes that happen atomically. Each change has different increasing sub revision in that set.
     sub int64
    }
    ```

    BoltDB 中存储的 key是**`reversion`**, value 则是`etcd` 自己的**`key-value组合`**，也就是说 etcd BoltDB 中保存每个版本，从而实现多版本机制。 例如：

    ```sh
    etcdctl txn <<<'
    put key1 "v1" put key2 "v2"
    '
    ```

    再次更新这两个key

    ```sh
    etcdctl txn <<<'
    put key1 "v11" put key2 "v22"
    '
    ```

    那么此时BoltDB中包含了4条数

    ```sh
    rev={3 0},key=key1,value="v1"
    rev={3 1},key=key2,value="v2"
    rev={4 0},key=key1,value="v11"
    rev={4 1},key=key2,value="v22"
    ```

    同一个事务的mainID是相同的，每进行一次操作suIDv会加1。第一次更新`mainID=3 subID=0` 第二次则`mainID=3 subID=1`。

    **compact**

    不过，这样的实现方式有一个很明显的问题，如果保存所有历史版本会导致数据库越来越大所以etcd提供了删除旧版本数据的方法:`Compact`。

    用户可以通过命令行工具即配置选项`手动`或`定时`的压缩老版本数据。

    **如何查询**

    客户端通过key来查询，可是实际boltDB保存的是reversion和 key-value。此时就需要有一个Key与reversion的映射，在etcd中就在内存中维护`kvindex`来存储。

    `kvindex`是google开源的golang版本**B树**(是的，就是B树)实现。

    所以用户查询时，只需要通过key，然后再内存的`kvindex`中找到对应的revision,然后通过revesion去boltDB查询。

6. **etcd的MVCC**

    etcd的MVCC数据存储分两部分: **内存**保存所有key对应的版本信息，用于快速**范围查询与点查**， 而**磁盘**存储所有不同版本的真实数据。

    存储在内存中的模型如下:

    ![8f98dd1b711894d3f270b120bb4929f3.png](https://img-blog.csdnimg.cn/img_convert/8f98dd1b711894d3f270b120bb4929f3.png)

    kvindex btree

    内存数据由 btree 来维护，从图上可以看到，key 是用户真实的 key, value 是对应所有的版本信息。

    ```go
    type keyIndex struct {
     key         []byte
     modified    revision // the main rev of the last modification
     generations []generation
    }
    
    // generation contains multiple revisions of a key.
    type generation struct {
     ver     int64
     created revision // when the generation is created (put in first revision).
     revs    []revision
    }
    ```

    `keyIndex` 保存 key 的所有版本信息，每删除一次都会生成一个 `generation`, 每个 `generation` 保存了这个生命周期内从创建到删除中间的所有版本号。

    存储在磁盘中的模型如下:

    ![dce4f0b52e5f6330efa501e4fbb93465.png](https://img-blog.csdnimg.cn/img_convert/dce4f0b52e5f6330efa501e4fbb93465.png)

    磁盘 **boltdb**

    磁盘负责存储所有数据，key 是 `revision`, value 是 `mvccpb.KeyValue`, 存储引擎是 boltdb

    ```go
    // protobuf
    type KeyValue struct {
     // key is the key in bytes. An empty key is not allowed.
     Key []byte `protobuf:"bytes,1,opt,name=key,proto3" json:"key,omitempty"`
     // create_revision is the revision of last creation on this key.
     CreateRevision int64 `protobuf:"varint,2,opt,name=create_revision,json=createRevision,proto3" json:"create_revision,omitempty"`
     // mod_revision is the revision of last modification on this key.
     ModRevision int64 `protobuf:"varint,3,opt,name=mod_revision,json=modRevision,proto3" json:"mod_revision,omitempty"`
     // version is the version of the key. A deletion resets
     // the version to zero and any modification of the key
     // increases its version.
     Version int64 `protobuf:"varint,4,opt,name=version,proto3" json:"version,omitempty"`
     // value is the value held by the key, in bytes.
     Value []byte `protobuf:"bytes,5,opt,name=value,proto3" json:"value,omitempty"`
     // lease is the ID of the lease that attached to key.
     // When the attached lease expires, the key will be deleted.
     // If lease is 0, then no lease is attached to the key.
     Lease int64 `protobuf:"varint,6,opt,name=lease,proto3" json:"lease,omitempty"`
    }
    ```

    `mvccpb.KeyValue` 存储本次操作的 key, value, 还有相关的所有版本信息。

7. **小结**

    etcd v3 MVCC 实现的基本原则就是：内存B树维护的是keyIndex,即用户 key到revision的映射，而revision又可以映射到磁盘 bbolt 的用户value中。

    ```markdown
         kvindex              boltDb
    key------------>revision---------->value
    ```

## 4. etcd应用场景

- **服务发现**

    ![img](https://images2015.cnblogs.com/blog/867003/201702/867003-20170214172422566-1801833324.jpg)

- **消息发布与订阅**

    ![img](https://images2015.cnblogs.com/blog/867003/201702/867003-20170214172544035-1529443473.jpg)

- **分布式锁**

    因为etcd使用Raft算法保持了数据的强一致性，某次操作存储到集群中的值必然是全局一致的，所以很容易实现分布式锁。锁服务有两种使用方式，一是保持独占，二是控制时序。

    (1) 准备

    客户端连接etcd,然后以某个名字(锁的名字)作为前缀，并创建一个全局唯一的ID，拼接后作为key存储。

    (2) 创建定时任务作为续租的"心跳"

    客户端持有锁的期间，其他客户端只能等待。持有锁的客户端会创建定时任务作为“心跳”不断对这个锁进行续租。如果持有锁的客户端奔溃，则会由于租约到期而自动释放锁。

    (3) 客户端将自己全局唯一ID写入etcd

    将步骤1中创建的key写入etcd,根据etcd 中的Revision 机制，假设两个客户端put操作返回的revision分别为1和2，客户端需要记录revision判断自己是否获得锁了。

    (4) 客户端判断是否获得锁

    客户端以前缀 /lock/mylock 读取 keyValue 列表（keyValue 中带有 key 对应的 Revision），判断自己 key 的 Revision 是否为当前列表中最小的，如果是则认为获得锁；否则监听列表中前一个 Revision 比自己小的 key 的删除事件，一旦监听到删除事件或者因租约失效而删除的事件，则自己获得锁。

    (5) 执行业务

    (6) 释放锁

## 5. docker

 1. **容器之间的通信方式**

    

# 2. web_server

## 1. 整体思路

![web_server note](https://cdn.jsdelivr.net/gh/dopamine-joker/image-host/image/web_server note.png)

## 2. 线程池while循环问题

这次不会使得cpu被占用，因为代码中有一句

```c++
m_queuestat.wait();
```

这里会休眠等待信号量来唤醒。

代码的主循环中采用epoll来实现多路复用，其中监听连接的fd特殊标记，每次处理fd都判断一下是否是监听的那个fd，是的话就处理连接。

# 3. [web服务器线程池数量设置](https://www.cnblogs.com/itplay/p/11372997.html)

1. 工作线程是不是越多越好

    不是，原因

    1) 服务器CPU核数有限，同时并发线程数有限，1核CPU设置10000个工作线程没有意义
    2) 线程切换有开销，如何线程切换过于频繁，反而会是性能降低

2. 调用sleep()函时，线程是否一直占用CPU？

    **不会**占用，等待时会把CPU让出来，给其他需要CPU资源的线程使用。

    不止sleep，在网络编程的阻塞accept()和阻塞recv()中都不会占用CPU资源

3. CPU单核设置多线程有意义吗

    单核设置多线程也是有意义的

    1) 多线程可以**让代码更加清晰**，有些IO线程收发包，有些worker线程进行任务处理，有些timeout线程进行超时检测

    2) 如果有一个任务一直占用CPU资源，此时增加线程并不能增加并发

        ```c++
        while(1) {
            i++;
        }
        ```

        该代码会不停占用CPU资源进行计算，使CPU资源占用率达到100%

4. 常见服务线程模型

    ![img](https://img2018.cnblogs.com/blog/1198522/201908/1198522-20190818165848833-1465909610.png)

    1）有少数几个IO线程监听上游发过来的请求，并进行收发包（生产者）
    2）有一个或者多个任务队列，作为IO线程与Worker线程异步解耦的数据传输通道（临界资源）
    3）有多个工作线程执行正真的任务（消费者）

5. 线程数设置的例子

    ![img](https://img2018.cnblogs.com/blog/1198522/201908/1198522-20190818165903669-1054379430.png)

    上图是一个典型的工作线程的处理过程，从开始处理start到结束处理end，该任务的处理共有7个步骤：
         1）从工作队列里拿出任务，进行一些本地初始化计算，例如http协议分析、参数解析、参数校验等
         2）访问cache拿一些数据
         3）拿到cache里的数据后，再进行一些本地计算，这些计算和业务逻辑相关
         4）通过RPC调用下游service再拿一些数据，或者让下游service去处理一些相关的任务
         5）RPC调用结束后，再进行一些本地计算，怎么计算和业务逻辑相关
         6）访问DB进行一些数据操作
        7）操作完数据库之后做一些收尾工作，同样这些收尾工作也是本地计算，和业务逻辑相关

    分析时间轴：

    1） 1，3，5，6线程本地业务逻辑处理需要占用的CPU时间

    2）2，4，6，访问cache，service，DB处于等待状态，不需要占用CPU

    假设上述中，本地处理时间和等待时间的比值为**1:1**，这时线程有50％的时间在计算(占用CPU)，有50%的时间在等待(不占用CPU)。

    <font color=red>**如果是单核**</font>，则设置**2个**工作线程即可充分利用，让CPU跑到100%，这样在第一个线程那50%的等待时间里，就可以调度另外一个线程进来工作。

    <font color=red>**如果是N核**</font>，则设置**2N**个工作线程即可充分利用CPU，让CPU跑到N*100%

    <font color=red size=7>结论</font>

    <font corlor=red size=6>N核的服务器，若本地执行业务的单线程本地时间为y,等待时间为x,则工作线程数(线程池线程数)设置为N*(x+y)/y，让CPU利用率最大化。</font>

    <font color=red size=7>经验</font>

    一般来说，非CPU密集型的业务（加解密、压缩解压缩、搜索排序等业务是CPU密集型的业务），瓶颈都在后端数据库，本地CPU计算的时间很少，所以设置几十或者几百个工作线程也都是可能的。

    **CPU密集型，线程数=核心数N+1**

    **I/O密集型，线程数=核心数N*2+1**

    **如果都存在，则分开两个线程池**

# 4. 游戏服务器设计

https://www.jianshu.com/p/84ab097df650`

# 5. [分布式事务](https://zhuanlan.zhihu.com/p/183753774)

(1) 2PC,3PC,TCC

(2) 本地消息表

(3) 消息事务

https://www.cnblogs.com/mayundalao/p/11798502.html
